{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazheng/miniconda3/envs/eff/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from torch.backends import cudnn\n",
    "from matplotlib import pyplot as plt\n",
    "from backbone import EfficientDetBackbone\n",
    "from efficientdet.utils import BBoxTransform, ClipBoxes\n",
    "from utils.utils import preprocess, invert_affine, postprocess, preprocess_video\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.vis_utils import model_params, model_load\n",
    "from utils.eff_utils import load_yaml\n",
    "\n",
    "yaml = load_yaml('datasets/detection/video/video_config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_src = \"datasets/detection/video/mov_clear/ch01_20220214162545.mp4\"  # set int to use webcam, set str to read from a video file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "compound_coef = 2\n",
    "force_input_size = None  # set None to use default size\n",
    "\n",
    "threshold = 0.2\n",
    "iou_threshold = 0.2\n",
    "\n",
    "use_cuda = True\n",
    "use_float16 = False\n",
    "cudnn.fastest = True\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# color_list = standard_to_bgr(STANDARD_COLORS)\n",
    "# tf bilinear interpolation is different from any other's, just make do\n",
    "input_sizes = [512, 640, 768, 896, 1024, 1280, 1280, 1536, 1536]\n",
    "input_size = (\n",
    "    input_sizes[compound_coef] if force_input_size is None else force_input_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = []\n",
    "for i, yaml_path in enumerate(yaml['project_config_list']):\n",
    "    project_params = load_yaml(str(yaml_path))\n",
    "    anchor_ratios = eval(project_params[\"anchors_ratios\"])\n",
    "    anchor_scales = eval(project_params[\"anchors_scales\"])\n",
    "    obj_list = project_params[\"obj_list\"]\n",
    "    params.append(\n",
    "        model_params(\n",
    "            yaml[\"compound_coef\"][i], obj_list, anchor_ratios, anchor_scales, yaml['weights_path_list'][i]\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[model_params(compound_coef=2, obj_list=['spiledmaterial', 'person', 'crack', 'potholes', 'pounding', 'label', 'indicator', 'lamplight', 'animal'], ratios=[(0.6, 1.6), (1.2, 0.8), (1.7, 0.6)], scales=[0.4, 0.5039684199579493, 0.6349604207872798], model_path='weights/untunnel/2.5/efficientdet-d2_99_6000.pth'),\n",
       " model_params(compound_coef=2, obj_list=['pounding'], ratios=[(1.3, 0.7), (1.7, 0.6), (2.1, 0.5)], scales=[0.5, 0.6299605249474366, 0.7937005259840997], model_path='logs/pounding/efficientdet-d2_199_12000.pth')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spiledmaterial',\n",
       " 'person',\n",
       " 'crack',\n",
       " 'potholes',\n",
       " 'pounding',\n",
       " 'label',\n",
       " 'indicator',\n",
       " 'lamplight',\n",
       " 'animal']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt_obj_list = params[0].obj_list\n",
    "plt_obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = []\n",
    "for i in range(len(params)):\n",
    "    model = model_load(params[i])\n",
    "    if use_cuda:\n",
    "        _model = model.cuda()\n",
    "    if use_float16:\n",
    "        _model = model.half()\n",
    "    model_list.append(_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(video_src)\n",
    "\n",
    "\n",
    "video_frame_cnt = int(cap.get(7))\n",
    "video_width = int(cap.get(3))\n",
    "video_height = int(cap.get(4))\n",
    "video_fps = int(cap.get(5))\n",
    "\n",
    "video_path = Path(video_src)\n",
    "# videoname = str(video_path.parents[1] / video_path.stem) + \"_output.avi\"\n",
    "videoname = \"test.avi\"\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"XVID\")\n",
    "writer = cv2.VideoWriter(videoname, fourcc, video_fps, (video_width, video_height), True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test.avi'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videoname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import (\n",
    "    preprocess,\n",
    "    invert_affine,\n",
    "    postprocess,\n",
    "    plot_one_box,\n",
    "    STANDARD_COLORS,\n",
    "    standard_to_bgr,\n",
    "    get_index_label,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_v2(preds, imgs, obj_list, exclusion_list=[]):\n",
    "    color_list = standard_to_bgr(STANDARD_COLORS)\n",
    "    plt.rcParams[\"figure.figsize\"] = (12.8, 7.2)\n",
    "    for i in range(len(imgs)):\n",
    "        if len(preds[i][\"rois\"]) == 0:\n",
    "            return imgs[i]\n",
    "\n",
    "        for j in range(len(preds[i][\"rois\"])):\n",
    "            (x1, y1, x2, y2) = preds[i][\"rois\"][j].astype(np.int)\n",
    "            obj = obj_list[preds[i][\"class_ids\"][j]]\n",
    "            score = float(preds[i][\"scores\"][j])\n",
    "\n",
    "            if obj not in exclusion_list:\n",
    "                cv2.rectangle(imgs[i], (x1, y1), (x2, y2), (255, 255, 0), 2)\n",
    "                plot_one_box(\n",
    "                    imgs[i],\n",
    "                    [x1, y1, x2, y2],\n",
    "                    label=obj,\n",
    "                    score=score,\n",
    "                    color=color_list[get_index_label(obj, obj_list)],\n",
    "                )\n",
    "\n",
    "        return imgs[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty = {\n",
    "                    \"rois\": np.array(()),\n",
    "                    \"class_ids\": np.array(()),\n",
    "                    \"scores\": np.array(()),\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_out(out_1,out_2):\n",
    "    out = out_1.copy()\n",
    "    for keys in out_1.keys():\n",
    "        if len(out_1['rois']) == 0:\n",
    "            out[keys] = out_2[keys]\n",
    "        elif len(out_2['rois']) == 0:\n",
    "            out[keys] = out_1[keys]\n",
    "        else:\n",
    "            out[keys] = np.concatenate((out_1[keys],out_2[keys]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list = ['pounding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_ind_list = [plt_obj_list.index(obj) for obj in obj_list]\n",
    "convert_ind_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_frame/all_frame:1/4051\n",
      "current_frame/all_frame:2/4051\n",
      "current_frame/all_frame:3/4051\n",
      "current_frame/all_frame:4/4051\n",
      "current_frame/all_frame:5/4051\n",
      "current_frame/all_frame:6/4051\n",
      "current_frame/all_frame:7/4051\n",
      "current_frame/all_frame:8/4051\n",
      "current_frame/all_frame:9/4051\n",
      "current_frame/all_frame:10/4051\n",
      "current_frame/all_frame:11/4051\n",
      "current_frame/all_frame:12/4051\n",
      "current_frame/all_frame:13/4051\n",
      "current_frame/all_frame:14/4051\n",
      "current_frame/all_frame:15/4051\n",
      "current_frame/all_frame:16/4051\n",
      "current_frame/all_frame:17/4051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mazheng/miniconda3/envs/eff/lib/python3.7/site-packages/ipykernel_launcher.py:9: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_frame/all_frame:18/4051\n",
      "current_frame/all_frame:19/4051\n",
      "current_frame/all_frame:20/4051\n",
      "current_frame/all_frame:21/4051\n",
      "current_frame/all_frame:22/4051\n",
      "current_frame/all_frame:23/4051\n",
      "current_frame/all_frame:24/4051\n",
      "current_frame/all_frame:25/4051\n",
      "current_frame/all_frame:26/4051\n",
      "current_frame/all_frame:27/4051\n",
      "current_frame/all_frame:28/4051\n",
      "current_frame/all_frame:29/4051\n",
      "current_frame/all_frame:30/4051\n",
      "current_frame/all_frame:31/4051\n",
      "current_frame/all_frame:32/4051\n",
      "current_frame/all_frame:33/4051\n",
      "current_frame/all_frame:34/4051\n",
      "current_frame/all_frame:35/4051\n",
      "current_frame/all_frame:36/4051\n",
      "current_frame/all_frame:37/4051\n",
      "current_frame/all_frame:38/4051\n",
      "current_frame/all_frame:39/4051\n",
      "current_frame/all_frame:40/4051\n",
      "current_frame/all_frame:41/4051\n",
      "current_frame/all_frame:42/4051\n",
      "current_frame/all_frame:43/4051\n",
      "current_frame/all_frame:44/4051\n",
      "current_frame/all_frame:45/4051\n",
      "current_frame/all_frame:46/4051\n",
      "current_frame/all_frame:47/4051\n",
      "current_frame/all_frame:48/4051\n",
      "current_frame/all_frame:49/4051\n",
      "current_frame/all_frame:50/4051\n",
      "current_frame/all_frame:51/4051\n",
      "current_frame/all_frame:52/4051\n",
      "current_frame/all_frame:53/4051\n",
      "current_frame/all_frame:54/4051\n",
      "current_frame/all_frame:55/4051\n",
      "current_frame/all_frame:56/4051\n",
      "current_frame/all_frame:57/4051\n",
      "current_frame/all_frame:58/4051\n",
      "current_frame/all_frame:59/4051\n",
      "current_frame/all_frame:60/4051\n",
      "current_frame/all_frame:61/4051\n",
      "current_frame/all_frame:62/4051\n",
      "current_frame/all_frame:63/4051\n",
      "current_frame/all_frame:64/4051\n",
      "current_frame/all_frame:65/4051\n",
      "current_frame/all_frame:66/4051\n",
      "current_frame/all_frame:67/4051\n",
      "current_frame/all_frame:68/4051\n",
      "current_frame/all_frame:69/4051\n",
      "current_frame/all_frame:70/4051\n",
      "current_frame/all_frame:71/4051\n",
      "current_frame/all_frame:72/4051\n",
      "current_frame/all_frame:73/4051\n",
      "current_frame/all_frame:74/4051\n",
      "current_frame/all_frame:75/4051\n",
      "current_frame/all_frame:76/4051\n",
      "current_frame/all_frame:77/4051\n",
      "current_frame/all_frame:78/4051\n",
      "current_frame/all_frame:79/4051\n",
      "current_frame/all_frame:80/4051\n",
      "current_frame/all_frame:81/4051\n",
      "current_frame/all_frame:82/4051\n",
      "current_frame/all_frame:83/4051\n",
      "current_frame/all_frame:84/4051\n",
      "current_frame/all_frame:85/4051\n",
      "current_frame/all_frame:86/4051\n",
      "current_frame/all_frame:87/4051\n",
      "current_frame/all_frame:88/4051\n",
      "current_frame/all_frame:89/4051\n",
      "current_frame/all_frame:90/4051\n",
      "current_frame/all_frame:91/4051\n",
      "current_frame/all_frame:92/4051\n",
      "current_frame/all_frame:93/4051\n",
      "current_frame/all_frame:94/4051\n",
      "current_frame/all_frame:95/4051\n",
      "current_frame/all_frame:96/4051\n",
      "current_frame/all_frame:97/4051\n",
      "current_frame/all_frame:98/4051\n",
      "current_frame/all_frame:99/4051\n",
      "current_frame/all_frame:100/4051\n",
      "current_frame/all_frame:101/4051\n",
      "current_frame/all_frame:102/4051\n",
      "current_frame/all_frame:103/4051\n",
      "current_frame/all_frame:104/4051\n",
      "current_frame/all_frame:105/4051\n",
      "current_frame/all_frame:106/4051\n",
      "current_frame/all_frame:107/4051\n",
      "current_frame/all_frame:108/4051\n",
      "current_frame/all_frame:109/4051\n",
      "current_frame/all_frame:110/4051\n",
      "current_frame/all_frame:111/4051\n",
      "current_frame/all_frame:112/4051\n",
      "current_frame/all_frame:113/4051\n",
      "current_frame/all_frame:114/4051\n",
      "current_frame/all_frame:115/4051\n",
      "current_frame/all_frame:116/4051\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20787/1949602035.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# frame preprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mori_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframed_imgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframed_metas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/waste-detection/Yet-Another-EfficientDet-Pytorch/utils/utils.py\u001b[0m in \u001b[0;36mpreprocess_video\u001b[0;34m(max_size, mean, std, *frame_from_video)\u001b[0m\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    104\u001b[0m     \u001b[0mori_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_from_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mnormalized_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mori_imgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     imgs_meta = [\n\u001b[1;32m    107\u001b[0m         \u001b[0maspectaware_resize_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/waste-detection/Yet-Another-EfficientDet-Pytorch/utils/utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    103\u001b[0m ):\n\u001b[1;32m    104\u001b[0m     \u001b[0mori_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe_from_video\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mnormalized_imgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mstd\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mori_imgs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     imgs_meta = [\n\u001b[1;32m    107\u001b[0m         \u001b[0maspectaware_resize_padding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressBoxes = BBoxTransform()\n",
    "clipBoxes = ClipBoxes()\n",
    "ind = 0\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    ind += 1\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # frame preprocessing\n",
    "    ori_imgs, framed_imgs, framed_metas = preprocess_video(frame, max_size=input_size)\n",
    "\n",
    "    if use_cuda:\n",
    "        x = torch.stack([torch.from_numpy(fi).cuda() for fi in framed_imgs], 0)\n",
    "    else:\n",
    "        x = torch.stack([torch.from_numpy(fi) for fi in framed_imgs], 0)\n",
    "\n",
    "    x = x.to(torch.float32 if not use_float16 else torch.float16).permute(0, 3, 1, 2)\n",
    "\n",
    "    # model predict\n",
    "    out_multi = [empty.copy()]\n",
    "    for i, model in enumerate(model_list):\n",
    "        obj_list = params[i].obj_list\n",
    "        convert_ind_list = [plt_obj_list.index(obj) for obj in obj_list]\n",
    "        with torch.no_grad():\n",
    "            features, regression, classification, anchors = model(x)\n",
    "\n",
    "            out = postprocess(\n",
    "                x,\n",
    "                anchors,\n",
    "                regression,\n",
    "                classification,\n",
    "                regressBoxes,\n",
    "                clipBoxes,\n",
    "                threshold,\n",
    "                iou_threshold,\n",
    "            )\n",
    "\n",
    "        # result\n",
    "        out = invert_affine(framed_metas, out)\n",
    "        for l,output in enumerate(out):\n",
    "            if len(output['class_ids']) != 0:\n",
    "                for j in output['class_ids']:\n",
    "                    if obj_list[j] in yaml['exclusion_list'][i]:\n",
    "                        output = empty.copy()\n",
    "                output['class_ids'] = np.array([convert_ind_list[i] for i in output['class_ids']])    \n",
    "                out_multi[l] = concat_out(out_multi[l],output)\n",
    "        # print(out_multi)\n",
    "    img_show = display_v2(out_multi, ori_imgs, plt_obj_list)\n",
    "\n",
    "    cv2.imwrite(\"datasets/detection/video/first.png\", img_show)\n",
    "    writer.write(img_show)\n",
    "    print(\"current_frame/all_frame:{}/{}\".format(ind, video_frame_cnt))\n",
    "    # if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "    #     break\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    img_show = display_v2(out_multi, ori_imgs, plt_obj_list)\n",
    "\n",
    "    cv2.imwrite(\"datasets/detection/video/first.png\", img_show)\n",
    "    writer.write(img_show)\n",
    "    print(\"current_frame/all_frame:{}/{}\".format(ind, video_frame_cnt))\n",
    "    # if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "    #     break\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        img_show = display_v2(out, ori_imgs, obj_list, yaml['exclusion_list'])\n",
    "\n",
    "    cv2.imwrite(\"datasets/detection/video/first.png\", img_show)\n",
    "    writer.write(img_show)\n",
    "    print(\"current_frame/all_frame:{}/{}\".format(ind, video_frame_cnt))\n",
    "    # if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "    #     break\n",
    "\n",
    "cap.release()\n",
    "writer.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_1=[{'rois': np.array([], dtype=np.float64), 'class_ids': np.array([], dtype=np.float64), 'scores': np.array([], dtype=np.float64)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_2=[{'rois': np.array([[ 587.80426,  585.9524 ,  628.3952 ,  678.4821 ],\n",
    "       [ 198.14296,  963.407  ,  372.13025, 1034.8644 ]], dtype=np.float32), 'class_ids': np.array([1, 2]), 'scores': np.array([0.60925406, 0.20762798], dtype=np.float32)}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rois': array([[ 587.80426,  585.9524 ,  628.3952 ,  678.4821 ],\n",
      "       [ 198.14296,  963.407  ,  372.13025, 1034.8644 ]], dtype=float32), 'class_ids': array([1, 2]), 'scores': array([0.60925406, 0.20762798], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "for j in out_2[0]['class_ids']:\n",
    "    j = 6\n",
    "print(out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'rois': array([[ 587.80426,  585.9524 ,  628.3952 ,  678.4821 ],\n",
      "       [ 198.14296,  963.407  ,  372.13025, 1034.8644 ]], dtype=float32), 'class_ids': array([2, 3]), 'scores': array([0.60925406, 0.20762798], dtype=float32)}]\n"
     ]
    }
   ],
   "source": [
    "out_2[0]['class_ids'] = np.array([i+1 for i in out_2[0]['class_ids']])\n",
    "print(out_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rois': array([[ 587.80426,  585.9524 ,  628.3952 ,  678.4821 ],\n",
       "        [ 198.14296,  963.407  ,  372.13025, 1034.8644 ]], dtype=float32),\n",
       " 'class_ids': array([1, 2]),\n",
       " 'scores': array([0.60925406, 0.20762798], dtype=float32)}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concat_out(out_1[0],out_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9e9b159799285ea164719ddc4fe351d6fa9b0332cb9ca4857e109bca51307c6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.0 ('eff')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
